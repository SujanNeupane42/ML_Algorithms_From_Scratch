{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5161831d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import random\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa719a6",
   "metadata": {},
   "source": [
    "# Descision Tree algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "60980920",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DescisionTree:\n",
    "  def __init__(self, min_samples_leaf, min_samples_split, max_depth, criterion, max_features, ml_task, lambdaa):\n",
    "    self.data = None\n",
    "    self.X = None\n",
    "    self.y = None\n",
    "    self.max_features = max_features\n",
    "    self.ml_task = ml_task\n",
    "    self.min_samples_leaf = min_samples_leaf\n",
    "    self.min_samples_split = min_samples_split\n",
    "    self.max_depth = max_depth\n",
    "    self.metric = criterion\n",
    "    self.feature_importances_ = None\n",
    "    self.complete_tree = None\n",
    "    self.n_entries = {}\n",
    "    self.n_weighted_entries = {}\n",
    "    self.parent_node = 1  # root node\n",
    "    self.yes_node = 2     # left node\n",
    "    self.no_node = 3      # right node\n",
    "    self.leaf_count = 0\n",
    "    self.lambdaa = lambdaa\n",
    "    if ml_task == 'classification': self.classes_and_counts = {};self.leaf_node_class_proba = {}\n",
    "    else: self.leaf_node_loss = {}\n",
    "    \n",
    "    \n",
    "\n",
    "  ''' This method is used to get the collective counts of all classes in target '''\n",
    "  def get_classes_and_counts(self, data):\n",
    "    label_column = data[:, -1]\n",
    "    unique_classes, counts_unique_classes = np.unique(label_column, return_counts=True)\n",
    "    for i in range(len(unique_classes)):\n",
    "      self.classes_and_counts[unique_classes[i]] = counts_unique_classes[i]\n",
    "\n",
    "  ''' This method is used to get the collective probabilities of all classes in target '''\n",
    "  def get_probability_for_all_classes(self, data):\n",
    "    label_column = data[:, -1]\n",
    "    unique_classes_new, counts_unique_classes_new = np.unique(label_column, return_counts=True)\n",
    "    \n",
    "    classes_and_counts_new = {}\n",
    "    for i in list(self.classes_and_counts.keys()):\n",
    "      if i in list(unique_classes_new):\n",
    "        classes_and_counts_new[i] = counts_unique_classes_new[list(unique_classes_new).index(i)]\n",
    "      else:\n",
    "        classes_and_counts_new[i] = 0\n",
    "    array = np.array(list(classes_and_counts_new.values())) / sum(classes_and_counts_new.values())\n",
    "\n",
    "    return [round(i, 5) for i in array]\n",
    "\n",
    "\n",
    "  ''' This method checks the purity of a target vector '''\n",
    "  def check_purity(self, data):\n",
    "      label_column = data[:, -1]\n",
    "      unique_classes = np.unique(label_column)\n",
    "      if len(unique_classes) == 1:\n",
    "          return True\n",
    "      else:\n",
    "          return False\n",
    "    \n",
    "#   def get_output_from_leaf(self, )\n",
    "\n",
    "\n",
    "  ''' This method performs classification '''\n",
    "  def create_leaf(self, data, ml_task, current_node):\n",
    "      self.leaf_count += 1\n",
    "      label_column = data[:, -1]\n",
    "      if ml_task == \"regression\":\n",
    "          leaf = self.get_similarity_score(data, 1)  # np.mean(label_column)\n",
    "          self.leaf_node_loss[current_node] = self.mse(data)\n",
    "          return str(leaf) + ' Node: '+str(current_node)\n",
    "      else:\n",
    "          probabilities = []\n",
    "          unique_classes, counts_unique_classes = np.unique(label_column, return_counts=True)\n",
    "          index = counts_unique_classes.argmax()\n",
    "          leaf = unique_classes[index]\n",
    "          # probability = counts_unique_classes[index] / sum(counts_unique_classes)\n",
    "          self.leaf_node_class_proba[current_node] = self.get_probability_for_all_classes(data)\n",
    "          \n",
    "          return str(leaf) + ' Node: '+str(current_node)\n",
    "    \n",
    "    \n",
    "  ''' this function generates all possible potential splits for a given training data '''\n",
    "  def get_potential_splits(self, data, random_subspace):  # randomly selecting certain features\n",
    "    potential_splits = {}\n",
    "    _, n_columns = data.shape\n",
    "    column_indices = list(range(n_columns - 1))    # excluding the last column which is the label\n",
    "    if random_subspace and random_subspace <= len(column_indices):\n",
    "        column_indices = random.sample(population=column_indices, k=random_subspace)\n",
    "    for column_index in column_indices:          \n",
    "        values = data[:, column_index]\n",
    "        unique_values = np.unique(values)\n",
    "        potential_splits[column_index] = unique_values\n",
    "\n",
    "    return potential_splits\n",
    "    \n",
    "    \n",
    "  ''' This function splits the data into two partitions: Yes and no cases'''\n",
    "  def split_data(self, data, split_column, split_value):\n",
    "      split_column_values = data[:, split_column]\n",
    "      type_of_feature = FEATURE_TYPES[split_column]\n",
    "\n",
    "      if type_of_feature == \"continuous\":\n",
    "          data_below = data[split_column_values <= split_value]\n",
    "          data_above = data[split_column_values >  split_value]\n",
    "      else:\n",
    "          data_below = data[split_column_values == split_value]\n",
    "          data_above = data[split_column_values != split_value]\n",
    "      \n",
    "      return data_below, data_above\n",
    "    \n",
    "    \n",
    "  ''' This method calculates mse loss'''\n",
    "  def mse(self, data):\n",
    "      actual_values = data[:, -1]\n",
    "      if len(actual_values) == 0:   # empty data\n",
    "          mse = 0\n",
    "      else:\n",
    "          prediction = np.mean(actual_values)\n",
    "          mse = np.mean((actual_values - prediction) **2)\n",
    "      \n",
    "      return mse\n",
    "  \n",
    "  def get_similarity_score(self, data, case):\n",
    "        label_column = data[:, -1]\n",
    "        numerator = np.sum(label_column) ** case\n",
    "        denominator = len(label_column) + self.lambdaa # a regularization technique that \n",
    "        similarity_score = numerator / denominator\n",
    "        return similarity_score\n",
    "        \n",
    "\n",
    "  ''' This method calculates entropy loss '''\n",
    "  def entropy(self, data):\n",
    "      label_column = data[:, -1]\n",
    "      _, counts = np.unique(label_column, return_counts=True)\n",
    "      probabilities = counts / counts.sum()\n",
    "      entropy = sum(probabilities * -np.log2(probabilities))\n",
    "      \n",
    "      return entropy\n",
    "\n",
    "\n",
    "  ''' This method calculates gini impurity'''\n",
    "  def gini(self, data):\n",
    "    label_column = data[:, -1]\n",
    "    _, counts = np.unique(label_column, return_counts=True)\n",
    "    probabilities = counts / counts.sum()\n",
    "    gini_index = 0\n",
    "\n",
    "    for i in probabilities:\n",
    "      gini_index += i ** 2\n",
    "    \n",
    "    return 1 - gini_index\n",
    "\n",
    "\n",
    "  ''' calculating total/weighed value of the used metric '''\n",
    "  def calculate_overall_metric(self, data_below, data_above, leaf_similarity_score,metric_function):\n",
    "    # i am using same method for getting similaruty score and output from the leaf\n",
    "    left_branch_similarity = self.get_similarity_score(data_below, 2) \n",
    "    right_branch_similarity = self.get_similarity_score(data_above, 2)\n",
    "    gain = left_branch_similarity + right_branch_similarity - leaf_similarity_score\n",
    "    \n",
    "    return gain\n",
    "    \n",
    "#     n = len(data_below) + len(data_above)\n",
    "#     p_data_below = len(data_below) / n\n",
    "#     p_data_above = len(data_above) / n\n",
    "#     # weighted MSE, RMSE, Gini, and entropy\n",
    "#     overall_metric =  (p_data_below * metric_function(self, data_below) \n",
    "#                      + p_data_above * metric_function(self, data_above))\n",
    "    \n",
    "#     return overall_metric\n",
    "\n",
    "\n",
    "  \n",
    "  ''' Determining which split is the best by using the metric '''\n",
    "  def determine_best_split(self, data, potential_splits, ml_task, criterion, leaf_similarity_score):\n",
    "    first_iteration = True\n",
    "    for column_index in potential_splits:\n",
    "        for value in potential_splits[column_index]:\n",
    "            data_below, data_above = self.split_data(data, split_column=column_index, split_value=value)\n",
    "            \n",
    "        \n",
    "            current_overall_gain = self.calculate_overall_metric(data_below, data_above, leaf_similarity_score,metric_function=criterion)\n",
    "            if first_iteration or current_overall_gain >= best_overall_gain:\n",
    "                first_iteration = False\n",
    "                best_overall_gain = current_overall_gain\n",
    "                best_split_column = column_index\n",
    "                best_split_value = value\n",
    "    \n",
    "    return best_split_column, best_split_value\n",
    "\n",
    "\n",
    "  ''' determining the type of a feature among all features '''\n",
    "  def determine_type_of_feature(self, df):\n",
    "    feature_types = []\n",
    "    n_unique_values_treshold = 10\n",
    "\n",
    "    for feature in df.columns:\n",
    "        if feature != \"label\":\n",
    "            unique_values = df[feature].unique()\n",
    "            example_value = unique_values[0]\n",
    "            if (isinstance(example_value, str)) or (len(unique_values) <= n_unique_values_treshold):\n",
    "                feature_types.append(\"categorical\")\n",
    "            else:\n",
    "                feature_types.append(\"continuous\")\n",
    "    \n",
    "    return feature_types\n",
    " \n",
    "    \n",
    "  ''' THIS IS THE MAIN RECURSIVE ALGORITHM FOR DESCISION TREE'''\n",
    "\n",
    "  def tree(self, df, ml_task, counter,current_node, min_samples_leaf, min_samples_split,max_depth, criterion, answer, max_features):\n",
    "\n",
    "    # When the tree starts, the dataframe is converteed to numpy array, the depth of the tree is checked using counter variable and all features types are detected\n",
    "    if counter == 0:\n",
    "        global COLUMN_HEADERS, FEATURE_TYPES\n",
    "        COLUMN_HEADERS = df.columns\n",
    "        FEATURE_TYPES = self.determine_type_of_feature(df)\n",
    "        data = df.values\n",
    "        criterion = getattr(DescisionTree, criterion)\n",
    "        leaf_similarity_score = self.get_similarity_score(data, 2)\n",
    "    else:\n",
    "        data = df      \n",
    "        leaf_similarity_score = self.get_similarity_score(data, 2)\n",
    "\n",
    "    # storing the length of data passed into the node\n",
    "    self.n_entries['Node: '+str(current_node)] = [len(df)]\n",
    "    # criterion = getattr(DescisionTree, criterion)\n",
    "\n",
    "    # storing the loss/mse/rmse/gini/entropy in a specific node\n",
    "    self.n_entries['Node: '+str(current_node)].append(criterion(self, data))\n",
    "    \n",
    "    # incrementing yes/left nodes and no/right nodes such that yes will be a even node and no will be a odd node repectively\n",
    "    if (answer == 'yes answer'):\n",
    "      self.yes_node += 2\n",
    "    elif (answer == 'no answer'):\n",
    "      self.no_node += 2 \n",
    "      \n",
    "    # checking if that target of the data passed is either pure, has minimum samples to create a leaf, or the depth of tree has reached its maximum depth\n",
    "    if (self.check_purity(data)) or (len(data) == min_samples_leaf) or (counter == max_depth):\n",
    "        leaf = self.create_leaf(data, ml_task, current_node) # creating the leaf\n",
    "        return leaf \n",
    "    # if above requirements to create a leaf are not met, two new nodes will be created recursively respectively.\n",
    "    else:    \n",
    "        counter += 1 # when two new nodes are created, the depth of three is also incremented\n",
    "        \n",
    "        # if the data is not yet pure, but has not minimum samples to perform the split, a leaf is created\n",
    "        if (len(data) < min_samples_split):\n",
    "          leaf = self.create_leaf(data, ml_task, current_node)\n",
    "          return leaf\n",
    "        else:\n",
    "          # getting the all possible splits, determining which split has least loss, and splitting the data into left and right nodes respectively\n",
    "          potential_splits = self.get_potential_splits(data, max_features)\n",
    "          split_column, split_value = self.determine_best_split(data, potential_splits, ml_task,criterion, leaf_similarity_score)\n",
    "          data_below, data_above = self.split_data(data, split_column, split_value)\n",
    "          \n",
    "          # if the data seperated into left and right nodes, but there is no data, instead of creating a node, a leaft is created\n",
    "          if len(data_below) == 0 or len(data_above) == 0:\n",
    "              leaf = self.create_leaf(data, ml_task, current_node)\n",
    "              return leaf\n",
    "          \n",
    "          # finding the type of a selected feature column and its name\n",
    "          feature_name = COLUMN_HEADERS[split_column]\n",
    "          type_of_feature = FEATURE_TYPES[split_column]\n",
    "\n",
    "#           # creating the tree questions\n",
    "          if type_of_feature == \"continuous\":\n",
    "              question = \"{} <= {} (Node: {})\".format(feature_name, split_value, current_node)\n",
    "          # feature is categorical\n",
    "          else:\n",
    "              question = \"{} = {} (Node: {})\".format(feature_name, split_value, current_node)\n",
    "\n",
    "          # instantiate sub-tree\n",
    "          sub_tree = {question: []}\n",
    "\n",
    "          # creating left and right nodes recursively\n",
    "          yes_answer = self.tree(data_below, ml_task, counter, self.yes_node,min_samples_leaf, min_samples_split,max_depth, criterion, 'yes answer', max_features)\n",
    "          no_answer = self.tree(data_above, ml_task, counter, self.no_node,min_samples_leaf,min_samples_split, max_depth, criterion, 'no answer', max_features)\n",
    "          \n",
    "          # if both left and right nodes are same, only taking one value for a leaf node\n",
    "          if yes_answer == no_answer:\n",
    "              sub_tree = yes_answer\n",
    "          else:\n",
    "            sub_tree[question].append(yes_answer)\n",
    "            sub_tree[question].append(no_answer)\n",
    "          \n",
    "          return sub_tree\n",
    "\n",
    "    \n",
    "\n",
    "  ''' making probability of the predictions using this tree '''\n",
    "  def predict_example_probability(self, example, tree):\n",
    "    question = list(tree.keys())[0]\n",
    "    feature_name, comparison_operator, value = question.split(\" \")[:3]\n",
    "\n",
    "    # ask question\n",
    "    if comparison_operator == \"<=\":\n",
    "        if example[feature_name] <= float(value):\n",
    "            answer = tree[question][0]\n",
    "        else:\n",
    "            answer = tree[question][1]\n",
    "    # feature is categorical\n",
    "    else:\n",
    "        if str(example[feature_name]) == value:\n",
    "            answer = tree[question][0]\n",
    "        else:\n",
    "            answer = tree[question][1]\n",
    "    # base case\n",
    "    if not isinstance(answer, dict):\n",
    "        return int(answer.split()[2])\n",
    "            \n",
    "    # recursive part\n",
    "    else:\n",
    "        residual_tree = answer\n",
    "        return self.predict_example_probability(example, residual_tree)\n",
    "\n",
    "    \n",
    "  ''' fitting our tree with training data '''\n",
    "  def fit(self, X,y):\n",
    "    self.X = X.copy()\n",
    "    self.y = y.copy()\n",
    "    self.X['label'] = self.y\n",
    "    self.data = self.X\n",
    "\n",
    "    if self.ml_task == 'classification':\n",
    "      self.get_classes_and_counts(self.data.values)\n",
    "#     self.parent_similarity_score = \n",
    "    self.complete_tree = self.tree(self.data, self.ml_task, 0,self.parent_node, self.min_samples_leaf, self.min_samples_split, self.max_depth, self.metric,'parent_node', self.max_features)\n",
    "    \n",
    "    # calculating weighted entries\n",
    "    for key, value in self.n_entries.items():\n",
    "      self.n_weighted_entries[key] = [value[0] / len(self.X), value[1]]\n",
    "\n",
    "    return self.complete_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a2dcc547",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "class XGBRegressor:\n",
    "  def __init__(self,learning_rate = 0.1,subsample = 1,criterion = 'mse', n_estimators=100, ml_task='regression', \\\n",
    "               max_depth=5, max_features= None, min_samples_leaf = 1, min_samples_split = 2, lambdaa = 1):\n",
    "    self.n_estimators = n_estimators\n",
    "    self.subsample = subsample\n",
    "    self.max_features = max_features\n",
    "    self.ml_task = ml_task\n",
    "    self.min_samples_leaf = min_samples_leaf\n",
    "    self.min_samples_split = min_samples_split\n",
    "    self.max_depth = max_depth\n",
    "    self.metric = criterion\n",
    "    self.forest = []\n",
    "    self.X = None\n",
    "    self.y = None\n",
    "    self.each_tree_with_class_probabilities = []\n",
    "    self.n_classes = None\n",
    "    self.learning_rate = learning_rate\n",
    "    self.lambdaa = lambdaa\n",
    "\n",
    "\n",
    "  ''' Calculating the negative first order derivative of MSE'''\n",
    "  def negative_first_order_derivative_of_mse(self,y,y_pred):\n",
    "        return (2 * (y - y_pred))\n",
    "\n",
    "\n",
    "  ''' Training our model '''\n",
    "  def fit(self, X, y):\n",
    "    self.X = X.copy(); self.y = y.copy()\n",
    "    self.initial_leaf_pred = np.full((X.shape[0]), y.mean())\n",
    "    \n",
    "    for i in tqdm(range(self.n_estimators)):\n",
    "        \n",
    "      pseudo_residuals = self.negative_first_order_derivative_of_mse(y, self.initial_leaf_pred)\n",
    "      tree = DescisionTree(min_samples_leaf = self.min_samples_leaf, min_samples_split = self.min_samples_split, max_depth =  self.max_depth, \\\n",
    "                           criterion = self.metric, max_features = self.max_features, ml_task = self.ml_task, lambdaa = self.lambdaa)\n",
    "      complete_tree = tree.fit(X, pseudo_residuals)\n",
    "      self.initial_leaf_pred += self.learning_rate * self.decision_tree_predictions(X, complete_tree)\n",
    "      self.forest.append(complete_tree)\n",
    "    \n",
    "    return 'Training completed'\n",
    "\n",
    "  ''' Making individual predictions '''\n",
    "  def predict_example(self, example, tree):\n",
    "    question = list(tree.keys())[0]\n",
    "    feature_name, comparison_operator, value = question.split(\" \")[:3]\n",
    "\n",
    "    # ask question\n",
    "    if comparison_operator == \"<=\":\n",
    "        if example[feature_name] <= float(value):\n",
    "            answer = tree[question][0]\n",
    "        else:\n",
    "            answer = tree[question][1]\n",
    "    # feature is categorical\n",
    "    else:\n",
    "        if str(example[feature_name]) == value:\n",
    "            answer = tree[question][0]\n",
    "        else:\n",
    "            answer = tree[question][1]\n",
    "    # base case\n",
    "    if not isinstance(answer, dict):\n",
    "        return float(answer.split()[0])\n",
    "    \n",
    "    # recursive part\n",
    "    else:\n",
    "        residual_tree = answer\n",
    "        return self.predict_example(example, residual_tree)\n",
    "\n",
    "\n",
    "\n",
    "  ''' Making predictions on testing dataframe '''\n",
    "  def decision_tree_predictions(self, test_df, tree):\n",
    "      predictions = test_df.apply(self.predict_example, args=(tree,), axis=1)\n",
    "      return predictions\n",
    "\n",
    "  \n",
    "    \n",
    "  ''' Method for making predictions ''' \n",
    "  def predict(self, test_df):\n",
    "      preds = np.full((test_df.shape[0]), self.y.mean())\n",
    "      for tree in self.forest:\n",
    "            preds += self.learning_rate * self.decision_tree_predictions(test_df, tree)\n",
    "      return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fc8095f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>children</th>\n",
       "      <th>smoker</th>\n",
       "      <th>region</th>\n",
       "      <th>charges</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>female</td>\n",
       "      <td>27.900</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>southwest</td>\n",
       "      <td>16884.92400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>male</td>\n",
       "      <td>33.770</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>southeast</td>\n",
       "      <td>1725.55230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>male</td>\n",
       "      <td>33.000</td>\n",
       "      <td>3</td>\n",
       "      <td>no</td>\n",
       "      <td>southeast</td>\n",
       "      <td>4449.46200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33</td>\n",
       "      <td>male</td>\n",
       "      <td>22.705</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>northwest</td>\n",
       "      <td>21984.47061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32</td>\n",
       "      <td>male</td>\n",
       "      <td>28.880</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>northwest</td>\n",
       "      <td>3866.85520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age     sex     bmi  children smoker     region      charges\n",
       "0   19  female  27.900         0    yes  southwest  16884.92400\n",
       "1   18    male  33.770         1     no  southeast   1725.55230\n",
       "2   28    male  33.000         3     no  southeast   4449.46200\n",
       "3   33    male  22.705         0     no  northwest  21984.47061\n",
       "4   32    male  28.880         0     no  northwest   3866.85520"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('insurance.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9467d304",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ad57ec42",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sex'] = encoder.fit_transform(df['sex'])\n",
    "df['smoker'] = encoder.fit_transform(df['smoker'])\n",
    "df['region'] = encoder.fit_transform(df['region'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "78bd0a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[df.columns.to_list()[:-1]]\n",
    "y = df['charges']\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state = 42)#, test_size = 0.32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ff0368be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:14<00:00,  6.93it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Training completed'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "booster = XGBRegressor(n_estimators = 100, max_depth=5, learning_rate = 0.1, lambdaa = 3)\n",
    "booster.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c5e31ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = booster.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "455b02ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RMSE 2609.143020326389\n",
      "Testing RMSE 4743.86599260462\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "print(\"Training RMSE\", np.sqrt(mean_squared_error(y_train, preds)))\n",
    "\n",
    "print(\"Testing RMSE\", np.sqrt(mean_squared_error(y_test, booster.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0db9dd74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RMSE 4064.402213347101\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "print(\"Training RMSE\", np.sqrt(mean_squared_error(y_train, preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "57db5bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing RMSE 4526.665608674652\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing RMSE\", np.sqrt(mean_squared_error(y_test, booster.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8bc08f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RMSE 3713.092115610719\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "print(\"Training RMSE\", np.sqrt(mean_squared_error(y_train, preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5b7e1c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing RMSE 4578.005337462994\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing RMSE\", np.sqrt(mean_squared_error(y_test, booster.predict(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac870b5d",
   "metadata": {},
   "source": [
    "# sklearn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f78228c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb5c2818",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(learning_rate=0.01, n_estimators=500)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "booster1 = GradientBoostingRegressor(n_estimators = 500, max_depth=3, learning_rate = 0.01,max_features= None, min_samples_leaf = 1, min_samples_split = 2)\n",
    "booster1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "90038f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RMSE 4062.3460252481586\n"
     ]
    }
   ],
   "source": [
    "print(\"Training RMSE\", np.sqrt(mean_squared_error(y_train, booster1.predict(X_train))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "19488bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing RMSE 4509.5479444432785\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing RMSE\", np.sqrt(mean_squared_error(y_test, booster1.predict(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bad664",
   "metadata": {},
   "source": [
    "# xg boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "11aeeaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "xgb = XGBRegressor(n_estimators = 100, max_depth = 5, learning_rate = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3ad0a085",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "             importance_type='gain', interaction_constraints='',\n",
       "             learning_rate=0.1, max_delta_step=0, max_depth=5,\n",
       "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "             n_estimators=100, n_jobs=12, num_parallel_tree=1, random_state=0,\n",
       "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "             tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "94d4e94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RMSE 2969.320781599348\n",
      "Testing RMSE 4660.307856781334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "print(\"Training RMSE\", np.sqrt(mean_squared_error(y_train, xgb.predict(X_train))))\n",
    "\n",
    "print(\"Testing RMSE\", np.sqrt(mean_squared_error(y_test, xgb.predict(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1829a215",
   "metadata": {},
   "source": [
    "# XGB for binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5109940c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DescisionTree:\n",
    "  def __init__(self, min_samples_leaf, min_samples_split, max_depth, criterion, max_features, ml_task, lambdaa, previous_probability_of_residuals):\n",
    "    self.data = None\n",
    "    self.X = None\n",
    "    self.y = None\n",
    "    self.max_features = max_features\n",
    "    self.ml_task = ml_task\n",
    "    self.min_samples_leaf = min_samples_leaf\n",
    "    self.min_samples_split = min_samples_split\n",
    "    self.max_depth = max_depth\n",
    "    self.metric = criterion\n",
    "    self.feature_importances_ = None\n",
    "    self.complete_tree = None\n",
    "    self.n_entries = {}\n",
    "    self.n_weighted_entries = {}\n",
    "    self.parent_node = 1  # root node\n",
    "    self.yes_node = 2     # left node\n",
    "    self.no_node = 3      # right node\n",
    "    self.leaf_count = 0\n",
    "    self.lambdaa = lambdaa\n",
    "    self.previous_probability_of_residuals = previous_probability_of_residuals\n",
    "    if ml_task == 'classification': self.classes_and_counts = {};self.leaf_node_class_proba = {}\n",
    "    else: self.leaf_node_loss = {}\n",
    "    \n",
    "    \n",
    "\n",
    "  ''' This method is used to get the collective counts of all classes in target '''\n",
    "  def get_classes_and_counts(self, data):\n",
    "    label_column = data[:, -1]\n",
    "    unique_classes, counts_unique_classes = np.unique(label_column, return_counts=True)\n",
    "    for i in range(len(unique_classes)):\n",
    "      self.classes_and_counts[unique_classes[i]] = counts_unique_classes[i]\n",
    "\n",
    "  ''' This method is used to get the collective probabilities of all classes in target '''\n",
    "  def get_probability_for_all_classes(self, data):\n",
    "    label_column = data[:, -1]\n",
    "    unique_classes_new, counts_unique_classes_new = np.unique(label_column, return_counts=True)\n",
    "    \n",
    "    classes_and_counts_new = {}\n",
    "    for i in list(self.classes_and_counts.keys()):\n",
    "      if i in list(unique_classes_new):\n",
    "        classes_and_counts_new[i] = counts_unique_classes_new[list(unique_classes_new).index(i)]\n",
    "      else:\n",
    "        classes_and_counts_new[i] = 0\n",
    "    array = np.array(list(classes_and_counts_new.values())) / sum(classes_and_counts_new.values())\n",
    "\n",
    "    return [round(i, 5) for i in array]\n",
    "\n",
    "\n",
    "  ''' This method checks the purity of a target vector '''\n",
    "  def check_purity(self, data):\n",
    "      label_column = data[:, -1]\n",
    "      unique_classes = np.unique(label_column)\n",
    "      if len(unique_classes) == 1:\n",
    "          return True\n",
    "      else:\n",
    "          return False\n",
    "    \n",
    "#   def get_output_from_leaf(self, )\n",
    "\n",
    "\n",
    "  ''' This method performs classification '''\n",
    "  def create_leaf(self, data, ml_task, current_node):\n",
    "      self.leaf_count += 1\n",
    "      label_column = data[:, -1]\n",
    "      if ml_task == \"regression\":\n",
    "          leaf = self.get_similarity_score(data, 1)  # np.mean(label_column)\n",
    "          self.leaf_node_loss[current_node] = self.mse(data)\n",
    "          return str(leaf) + ' Node: '+str(current_node)\n",
    "      else:\n",
    "          probabilities = []\n",
    "          unique_classes, counts_unique_classes = np.unique(label_column, return_counts=True)\n",
    "          index = counts_unique_classes.argmax()\n",
    "          leaf = unique_classes[index]\n",
    "          # probability = counts_unique_classes[index] / sum(counts_unique_classes)\n",
    "          self.leaf_node_class_proba[current_node] = self.get_probability_for_all_classes(data)\n",
    "          \n",
    "          return str(leaf) + ' Node: '+str(current_node)\n",
    "    \n",
    "    \n",
    "  ''' this function generates all possible potential splits for a given training data '''\n",
    "  def get_potential_splits(self, data, random_subspace):  # randomly selecting certain features\n",
    "    potential_splits = {}\n",
    "    _, n_columns = data.shape\n",
    "    column_indices = list(range(n_columns - 1))    # excluding the last column which is the label\n",
    "    if random_subspace and random_subspace <= len(column_indices):\n",
    "        column_indices = random.sample(population=column_indices, k=random_subspace)\n",
    "    for column_index in column_indices:          \n",
    "        values = data[:, column_index]\n",
    "        unique_values = np.unique(values)\n",
    "        potential_splits[column_index] = unique_values\n",
    "\n",
    "    return potential_splits\n",
    "    \n",
    "    \n",
    "  ''' This function splits the data into two partitions: Yes and no cases'''\n",
    "  def split_data(self, data, split_column, split_value):\n",
    "      split_column_values = data[:, split_column]\n",
    "      type_of_feature = FEATURE_TYPES[split_column]\n",
    "\n",
    "      if type_of_feature == \"continuous\":\n",
    "          data_below = data[split_column_values <= split_value]\n",
    "          data_above = data[split_column_values >  split_value]\n",
    "      else:\n",
    "          data_below = data[split_column_values == split_value]\n",
    "          data_above = data[split_column_values != split_value]\n",
    "      \n",
    "      return data_below, data_above\n",
    "    \n",
    "    \n",
    "  ''' This method calculates mse loss'''\n",
    "  def mse(self, data):\n",
    "      actual_values = data[:, -1]\n",
    "      if len(actual_values) == 0:   # empty data\n",
    "          mse = 0\n",
    "      else:\n",
    "          prediction = np.mean(actual_values)\n",
    "          mse = np.mean((actual_values - prediction) **2)\n",
    "      \n",
    "      return mse\n",
    "  \n",
    "  def get_similarity_score(self, data, case):\n",
    "        label_column = data[:, -1]\n",
    "        numerator = np.sum(label_column) ** case\n",
    "        denominator = 0 \n",
    "        for x in label_column:\n",
    "            prob = self.previous_probability_of_residuals[x]\n",
    "            denominator += prob * (1 - prob)\n",
    "        denominator += self.lambdaa  # adding the regularization\n",
    "        similarity_score = numerator / denominator\n",
    "        return similarity_score\n",
    "        \n",
    "\n",
    "  ''' This method calculates entropy loss '''\n",
    "  def entropy(self, data):\n",
    "      label_column = data[:, -1]\n",
    "      _, counts = np.unique(label_column, return_counts=True)\n",
    "      probabilities = counts / counts.sum()\n",
    "      entropy = sum(probabilities * -np.log2(probabilities))\n",
    "      \n",
    "      return entropy\n",
    "\n",
    "\n",
    "  ''' This method calculates gini impurity'''\n",
    "  def gini(self, data):\n",
    "    label_column = data[:, -1]\n",
    "    _, counts = np.unique(label_column, return_counts=True)\n",
    "    probabilities = counts / counts.sum()\n",
    "    gini_index = 0\n",
    "\n",
    "    for i in probabilities:\n",
    "      gini_index += i ** 2\n",
    "    \n",
    "    return 1 - gini_index\n",
    "\n",
    "\n",
    "  ''' calculating total/weighed value of the used metric '''\n",
    "  def calculate_overall_metric(self, data_below, data_above, leaf_similarity_score,metric_function):\n",
    "    # i am using same method for getting similaruty score and output from the leaf\n",
    "    left_branch_similarity = self.get_similarity_score(data_below, 2) \n",
    "    right_branch_similarity = self.get_similarity_score(data_above, 2)\n",
    "    gain = left_branch_similarity + right_branch_similarity - leaf_similarity_score\n",
    "    \n",
    "    return gain\n",
    "    \n",
    "#     n = len(data_below) + len(data_above)\n",
    "#     p_data_below = len(data_below) / n\n",
    "#     p_data_above = len(data_above) / n\n",
    "#     # weighted MSE, RMSE, Gini, and entropy\n",
    "#     overall_metric =  (p_data_below * metric_function(self, data_below) \n",
    "#                      + p_data_above * metric_function(self, data_above))\n",
    "    \n",
    "#     return overall_metric\n",
    "\n",
    "\n",
    "  \n",
    "  ''' Determining which split is the best by using the metric '''\n",
    "  def determine_best_split(self, data, potential_splits, ml_task, criterion, leaf_similarity_score):\n",
    "    first_iteration = True\n",
    "    for column_index in potential_splits:\n",
    "        for value in potential_splits[column_index]:\n",
    "            data_below, data_above = self.split_data(data, split_column=column_index, split_value=value)\n",
    "            \n",
    "        \n",
    "            current_overall_gain = self.calculate_overall_metric(data_below, data_above, leaf_similarity_score,metric_function=criterion)\n",
    "            if first_iteration or current_overall_gain >= best_overall_gain:\n",
    "                first_iteration = False\n",
    "                best_overall_gain = current_overall_gain\n",
    "                best_split_column = column_index\n",
    "                best_split_value = value\n",
    "    \n",
    "    return best_split_column, best_split_value\n",
    "\n",
    "\n",
    "  ''' determining the type of a feature among all features '''\n",
    "  def determine_type_of_feature(self, df):\n",
    "    feature_types = []\n",
    "    n_unique_values_treshold = 10\n",
    "\n",
    "    for feature in df.columns:\n",
    "        if feature != \"label\":\n",
    "            unique_values = df[feature].unique()\n",
    "            example_value = unique_values[0]\n",
    "            if (isinstance(example_value, str)) or (len(unique_values) <= n_unique_values_treshold):\n",
    "                feature_types.append(\"categorical\")\n",
    "            else:\n",
    "                feature_types.append(\"continuous\")\n",
    "    \n",
    "    return feature_types\n",
    " \n",
    "    \n",
    "  ''' THIS IS THE MAIN RECURSIVE ALGORITHM FOR DESCISION TREE'''\n",
    "\n",
    "  def tree(self, df, ml_task, counter,current_node, min_samples_leaf, min_samples_split,max_depth, criterion, answer, max_features):\n",
    "\n",
    "    # When the tree starts, the dataframe is converteed to numpy array, the depth of the tree is checked using counter variable and all features types are detected\n",
    "    if counter == 0:\n",
    "        global COLUMN_HEADERS, FEATURE_TYPES\n",
    "        COLUMN_HEADERS = df.columns\n",
    "        FEATURE_TYPES = self.determine_type_of_feature(df)\n",
    "        data = df.values\n",
    "        criterion = getattr(DescisionTree, criterion)\n",
    "        leaf_similarity_score = self.get_similarity_score(data, 2)\n",
    "    else:\n",
    "        data = df      \n",
    "        leaf_similarity_score = self.get_similarity_score(data, 2)\n",
    "\n",
    "    # storing the length of data passed into the node\n",
    "    self.n_entries['Node: '+str(current_node)] = [len(df)]\n",
    "    # criterion = getattr(DescisionTree, criterion)\n",
    "\n",
    "    # storing the loss/mse/rmse/gini/entropy in a specific node\n",
    "    self.n_entries['Node: '+str(current_node)].append(criterion(self, data))\n",
    "    \n",
    "    # incrementing yes/left nodes and no/right nodes such that yes will be a even node and no will be a odd node repectively\n",
    "    if (answer == 'yes answer'):\n",
    "      self.yes_node += 2\n",
    "    elif (answer == 'no answer'):\n",
    "      self.no_node += 2 \n",
    "      \n",
    "    # checking if that target of the data passed is either pure, has minimum samples to create a leaf, or the depth of tree has reached its maximum depth\n",
    "    if (self.check_purity(data)) or (len(data) == min_samples_leaf) or (counter == max_depth):\n",
    "        leaf = self.create_leaf(data, ml_task, current_node) # creating the leaf\n",
    "        return leaf \n",
    "    # if above requirements to create a leaf are not met, two new nodes will be created recursively respectively.\n",
    "    else:    \n",
    "        counter += 1 # when two new nodes are created, the depth of three is also incremented\n",
    "        \n",
    "        # if the data is not yet pure, but has not minimum samples to perform the split, a leaf is created\n",
    "        if (len(data) < min_samples_split):\n",
    "          leaf = self.create_leaf(data, ml_task, current_node)\n",
    "          return leaf\n",
    "        else:\n",
    "          # getting the all possible splits, determining which split has least loss, and splitting the data into left and right nodes respectively\n",
    "          potential_splits = self.get_potential_splits(data, max_features)\n",
    "          split_column, split_value = self.determine_best_split(data, potential_splits, ml_task,criterion, leaf_similarity_score)\n",
    "          data_below, data_above = self.split_data(data, split_column, split_value)\n",
    "          \n",
    "          # if the data seperated into left and right nodes, but there is no data, instead of creating a node, a leaft is created\n",
    "          if len(data_below) == 0 or len(data_above) == 0:\n",
    "              leaf = self.create_leaf(data, ml_task, current_node)\n",
    "              return leaf\n",
    "          \n",
    "          # finding the type of a selected feature column and its name\n",
    "          feature_name = COLUMN_HEADERS[split_column]\n",
    "          type_of_feature = FEATURE_TYPES[split_column]\n",
    "\n",
    "#           # creating the tree questions\n",
    "          if type_of_feature == \"continuous\":\n",
    "              question = \"{} <= {} (Node: {})\".format(feature_name, split_value, current_node)\n",
    "          # feature is categorical\n",
    "          else:\n",
    "              question = \"{} = {} (Node: {})\".format(feature_name, split_value, current_node)\n",
    "\n",
    "          # instantiate sub-tree\n",
    "          sub_tree = {question: []}\n",
    "\n",
    "          # creating left and right nodes recursively\n",
    "          yes_answer = self.tree(data_below, ml_task, counter, self.yes_node,min_samples_leaf, min_samples_split,max_depth, criterion, 'yes answer', max_features)          no_answer = self.tree(data_above, ml_task, counter, self.no_node,min_samples_leaf,min_samples_split, max_depth, criterion, 'no answer', max_features)\n",
    "          \n",
    "          # if both left and right nodes are same, only taking one value for a leaf node\n",
    "          if yes_answer == no_answer:\n",
    "              sub_tree = yes_answer\n",
    "          else:\n",
    "            sub_tree[question].append(yes_answer)\n",
    "            sub_tree[question].append(no_answer)\n",
    "          \n",
    "          return sub_tree\n",
    "\n",
    "    \n",
    "\n",
    "  ''' making probability of the predictions using this tree '''\n",
    "  def predict_example_probability(self, example, tree):\n",
    "    question = list(tree.keys())[0]\n",
    "    feature_name, comparison_operator, value = question.split(\" \")[:3]\n",
    "\n",
    "    # ask question\n",
    "    if comparison_operator == \"<=\":\n",
    "        if example[feature_name] <= float(value):\n",
    "            answer = tree[question][0]\n",
    "        else:\n",
    "            answer = tree[question][1]\n",
    "    # feature is categorical\n",
    "    else:\n",
    "        if str(example[feature_name]) == value:\n",
    "            answer = tree[question][0]\n",
    "        else:\n",
    "            answer = tree[question][1]\n",
    "    # base case\n",
    "    if not isinstance(answer, dict):\n",
    "        return int(answer.split()[2])\n",
    "            \n",
    "    # recursive part\n",
    "    else:\n",
    "        residual_tree = answer\n",
    "        return self.predict_example_probability(example, residual_tree)\n",
    "\n",
    "    \n",
    "  ''' fitting our tree with training data '''\n",
    "  def fit(self, X,y):\n",
    "    self.X = X.copy()\n",
    "    self.y = y.copy()\n",
    "    self.X['label'] = self.y\n",
    "    self.data = self.X\n",
    "\n",
    "    if self.ml_task == 'classification':\n",
    "      self.get_classes_and_counts(self.data.values)\n",
    "#     self.parent_similarity_score = \n",
    "    self.complete_tree = self.tree(self.data, self.ml_task, 0,self.parent_node, self.min_samples_leaf, self.min_samples_split, self.max_depth, self.metric,'parent_node', self.max_features)\n",
    "    \n",
    "    # calculating weighted entries\n",
    "    for key, value in self.n_entries.items():\n",
    "      self.n_weighted_entries[key] = [value[0] / len(self.X), value[1]]\n",
    "\n",
    "    return self.complete_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4bb6d715",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "class XGBClassifier:\n",
    "  def __init__(self,learning_rate = 0.1,subsample = 1,criterion = 'mse', n_estimators=100, ml_task='regression', \\\n",
    "               max_depth=5, max_features= None, min_samples_leaf = 1, min_samples_split = 2, lambdaa = 5):\n",
    "    self.n_estimators = n_estimators\n",
    "    self.subsample = subsample\n",
    "    self.max_features = max_features\n",
    "    self.ml_task = ml_task\n",
    "    self.min_samples_leaf = min_samples_leaf\n",
    "    self.min_samples_split = min_samples_split\n",
    "    self.max_depth = max_depth\n",
    "    self.metric = criterion\n",
    "    self.forest = []\n",
    "    self.X = None\n",
    "    self.y = None\n",
    "    self.each_tree_with_class_probabilities = []\n",
    "    self.n_classes = None\n",
    "    self.learning_rate = learning_rate\n",
    "    self.lambdaa = lambdaa\n",
    "\n",
    "\n",
    "  def sigmoid(self, x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "  def negativeDerivitiveLogloss(self, y, log_odds):\n",
    "    p = self.sigmoid(log_odds)\n",
    "    return (y - p)\n",
    "\n",
    "  def log_odds(self, y):\n",
    "    positive_count = np.count_nonzero(df.label.values == 1)\n",
    "    negative_count = np.count_nonzero(df.label.values == 0)\n",
    "    log_odds_value = np.log(positive_count / negative_count)\n",
    "\n",
    "    return np.full((y.shape[0]), log_odds_value)\n",
    "\n",
    "\n",
    "  ''' Training our model '''\n",
    "  def fit(self, X, y):\n",
    "    self.X = X.copy(); self.y = y.copy()\n",
    "    self.initial_leaf_pred = np.full((X.shape[0]), y.mean())\n",
    "    self.previous_probability_of_residuals = []\n",
    "    self.unique_residuals = []\n",
    "    \n",
    "    for i in tqdm(range(self.n_estimators)):\n",
    "      pseudo_residuals = self.negativeDerivitiveLogloss(y, self.initial_leaf_pred)\n",
    "      previous_probability_of_residuals = {}\n",
    "      previous_probability = [self.sigmoid(j) for j in self.initial_leaf_pred]\n",
    "    \n",
    "      for key, value in zip(pseudo_residuals, previous_probability):\n",
    "          previous_probability_of_residuals[key] = value  \n",
    "      \n",
    "      tree = DescisionTree(min_samples_leaf = self.min_samples_leaf, min_samples_split = self.min_samples_split, max_depth =  self.max_depth, \\\n",
    "                           criterion = self.metric, max_features = self.max_features, \\\n",
    "                           ml_task = self.ml_task, lambdaa = self.lambdaa,  previous_probability_of_residuals = previous_probability_of_residuals)\n",
    "      complete_tree = tree.fit(X, pseudo_residuals)\n",
    "      self.initial_leaf_pred += self.learning_rate * self.decision_tree_predictions(X, complete_tree)\n",
    "      self.forest.append(complete_tree)\n",
    "    \n",
    "    return 'Training completed'\n",
    "\n",
    "  ''' Making individual predictions '''\n",
    "  def predict_example(self, example, tree):\n",
    "    question = list(tree.keys())[0]\n",
    "    feature_name, comparison_operator, value = question.split(\" \")[:3]\n",
    "\n",
    "    # ask question\n",
    "    if comparison_operator == \"<=\":\n",
    "        if example[feature_name] <= float(value):\n",
    "            answer = tree[question][0]\n",
    "        else:\n",
    "            answer = tree[question][1]\n",
    "    # feature is categorical\n",
    "    else:\n",
    "        if str(example[feature_name]) == value:\n",
    "            answer = tree[question][0]\n",
    "        else:\n",
    "            answer = tree[question][1]\n",
    "    # base case\n",
    "    if not isinstance(answer, dict):\n",
    "        return float(answer.split()[0])\n",
    "    \n",
    "    # recursive part\n",
    "    else:\n",
    "        residual_tree = answer\n",
    "        return self.predict_example(example, residual_tree)\n",
    "\n",
    "\n",
    "\n",
    "  ''' Making predictions on testing dataframe '''\n",
    "  def decision_tree_predictions(self, test_df, tree):\n",
    "      predictions = test_df.apply(self.predict_example, args=(tree,), axis=1)\n",
    "      return predictions\n",
    "\n",
    "  \n",
    "    \n",
    "  def get_clean_output(self, preds):\n",
    "    return [1 if (i >= 0.5) else 0 for i in preds]\n",
    "    \n",
    "  ''' Method for making predictions ''' \n",
    "  def predict(self, test_df):\n",
    "      preds = np.full((test_df.shape[0]), self.log_odds(self.y.values)[0])\n",
    "      for tree in self.forest:\n",
    "            preds += self.learning_rate * self.decision_tree_predictions(test_df, tree)\n",
    "            \n",
    "      preds = [self.sigmoid(i) for i in preds]\n",
    "      return self.get_clean_output(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04624d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "df[\"label\"] = df.Survived\n",
    "df = df.drop([\"PassengerId\", \"Survived\", \"Name\", \"Ticket\", \"Cabin\"], axis=1)\n",
    "\n",
    "# handling missing values\n",
    "median_age = df.Age.median()\n",
    "mode_embarked = df.Embarked.mode()[0]\n",
    "\n",
    "df = df.fillna({\"Age\": median_age, \"Embarked\": mode_embarked})\n",
    "df.Sex = df.Sex.replace({'male':0,'female':1})\n",
    "df.Embarked = df.Embarked.replace({'S':0,'C':1,'Q':2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9a94897",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[df.columns.to_list()[:-1]]\n",
    "y = df['label']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.25, random_state = 4221314)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f67bb347",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.14it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Training completed'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "booster1 = XGBClassifier(n_estimators = 5, max_depth=3, learning_rate = 0.1,max_features= None, min_samples_leaf = 1, min_samples_split = 2)\n",
    "booster1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae53f22f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7994011976047904\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7757847533632287"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_train, booster1.predict(X_train)))\n",
    "accuracy_score(y_test, booster1.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd5533c",
   "metadata": {},
   "source": [
    "# XGBoost classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5d726a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:42:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.1, max_delta_step=0, max_depth=5,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=5, n_jobs=12, num_parallel_tree=1, random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "xgb = XGBClassifier(n_estimators = 5, max_depth = 5, learning_rate = 0.1)\n",
    "xgb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f25b9a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8712574850299402\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8251121076233184"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_train, xgb.predict(X_train)))\n",
    "accuracy_score(y_test, xgb.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e25789",
   "metadata": {},
   "source": [
    "# Final check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf06b85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('pima-indians-diabetes.csv', names = ['a','b','c','d','e','f','g','h','label'])\n",
    "X = df[df.columns.to_list()[:-1]]\n",
    "y = df['label']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.25, random_state = 4221314)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b9221856",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [02:20<00:00,  1.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.7552083333333334\n"
     ]
    }
   ],
   "source": [
    "booster1 = XGBClassifier(n_estimators = 100, max_depth=6, learning_rate = 0.3,max_features= None, min_samples_leaf = 1, min_samples_split = 2)\n",
    "booster1.fit(X_train, y_train)\n",
    "print(accuracy_score(y_train, booster1.predict(X_train)))\n",
    "print(accuracy_score(y_test, booster1.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5cea83f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:03:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.9913194444444444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\xgboost\\data.py:208: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.78125"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "xgb = XGBClassifier(n_estimators = 100, max_depth = 5, learning_rate = 0.1)\n",
    "xgb.fit(X_train, y_train)\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_train, xgb.predict(X_train)))\n",
    "accuracy_score(y_test, xgb.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6240b986",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
